{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预训练模型的特定：\n",
    "- 通用的特征提取器：不需要考虑如何提取语料特征\n",
    "- 动态化的词向量：NNLM，word2vec，Glove模型，针对每个特定字词所生成的词向量是一成不变的，无法对于存在一词多义等问题。预训练模型借助attention机制，就能够动态生成这种基于上下文的词向量。\n",
    "- 针对不同下游任务的强泛化性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）transformer是主流预训练模型的基本单元，是一个强大的特征提取器，后续的模型大多是基于transformer的优化和改进。\n",
    "\n",
    "（2）ELMo基于传统的RNN模型框架，是一个双向模型。它会根据上下文语境，动态地生成词向量。\n",
    "\n",
    "（3）BERT可以理解为集合了transformer优点（特征提取）和ELMo优点（双向）的模型，是最经典的预训练模型，是预训练模型的里程碑。\n",
    "\n",
    "（4）transformer-XL将transformer作为基本单元应用于RNN模型，改善了transformer输入序列长度受限（长文本）的问题。\n",
    "\n",
    "（5）XLNet是基于transformer-XL的模型，论文的主要创新点是双流机制和序列乱序方法。XLNet同时也使用了大量的语料，构建了更大的模型，也取得了比BERT更好的效果。但是通过大语料、大模型这种类似军备竞赛的方式刷新SOTA的方法，也逐渐开始泛滥。\n",
    "\n",
    "（6）ERNIE是百度提出的模型，它基于BERT，百度利用其强大的语料库优势进行训练，把ERNIE推到了中文NLP任务SOTA的地位，并保持了很长一段时间。\n",
    "\n",
    "（7）ERNIE2.0在ERNIE的基础上，使用多任务学习的方法，进一步提升了模型的泛化性和通用性。\n",
    "\n",
    "（8）GPT是一个早期模型，可以理解为将RNN的基本单元替换为transformer，是一个单向模型。\n",
    "\n",
    "（9）GPT-2也是一个使用大语料训练的大模型，它在文本生成任务上的表现让人惊艳了一把，也因为不肯开源被学术界工业界口诛笔伐（现在已经开源了）。\n",
    "\n",
    "（10）RoBERTa在BERT的基础上，致力于将调参发挥到极致。\n",
    "\n",
    "（11）ALBERT使用词向量因式分解和层间参数共享两个方法，在降低模型规模方向上，做了显著有效的尝试。\n",
    "\n",
    "（12）TinyBERT利用知识蒸馏技术降低模型规模，也获得了很好的效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
